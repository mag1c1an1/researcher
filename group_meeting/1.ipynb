{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概括\n",
    "朴素贝叶斯(naive Bayes)法是基于 贝叶斯定理 与 特征条件独立假设的分类方法。  \n",
    "对于给定的训练数据集，首先基于特征条件独立假设 学习 输入输出的联合概率分布；  \n",
    "然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。\n",
    "\n",
    "`贝叶斯定理`： $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "\n",
    "输入实例x的特征向量记作 $x = (x^{(1)},x^{(2)},...,x^{(i)},...,x^{(n)})^T$  \n",
    "$x^{(i)}$表示$\\mathcal{x}$的第i个特征, $x_i$表示多个输入变量中的第i个变量  \n",
    "\n",
    "核心思想：选择高概率对应的类别。\n",
    "\n",
    "`先验概率（prior probability）`：指根据以往经验和分析。在实验或采样前就可以得到的概率。  \n",
    "`后验概率（posterior probability）`：指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。\n",
    "\n",
    "`全概率公式`:  \n",
    "设事件$B_1,B_2,...,B_n$构成一个完备事件组，即它们两两不相容，和为全集且$P(B_i)>0$,则对任一事件A有：  \n",
    "$$P(A)=\\displaystyle\\sum_{i=1}^nP(B_i)P(A|B_i)$$\n",
    "\n",
    "##### 极大似然估计\n",
    "通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！\n",
    "\n",
    "##### 经验风险最小化\n",
    "经验风险最小化 （ERM）是统计学习理论里的一项原则，该原则下有一系列学习算法 ，经验风险最小化用于为这些算法的性能提供理论上的界。核心思想是，我们无法确切知道算法在实际中的运行情况（真正的“风险”），因为我们不知道算法将在其上运行的数据的真实分布，但借助经验风险最小化，我们可以在一组已知的训练数据（“经验”风险）上衡量其性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例1  \n",
    "$P(Y=1) = \\frac{9}{15}$,\n",
    "$P(Y=-1) = \\frac{6}{15}$  \n",
    "$P(X^{(1)}|Y=1)= \\frac{P(X^{(1)}=1)P(Y=1|X^{(1)}=1)}{P(Y=1)}= \\frac{\\frac{1}{3} \\cdot \\frac{2}{5}}{\\frac{9}{15}} = \\frac{2}{9}$  \n",
    "$P(Y=c_k)\\displaystyle\\prod_{j=1}^nP(X^{j}=x^{j}|Y=c_k), k = 1,2,...,K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res is -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "dt = {'S':0,'M':1,'L':2}\n",
    "X = np.array([\n",
    "    [1,dt['S']],\n",
    "    [1,dt['M']],\n",
    "    [1,dt['M']],\n",
    "    [1,dt['S']],\n",
    "    [1,dt['S']],\n",
    "    [2,dt['S']],\n",
    "    [2,dt['M']],\n",
    "    [2,dt['M']],\n",
    "    [2,dt['L']],\n",
    "    [2,dt['L']],\n",
    "    [3,dt['L']],\n",
    "    [3,dt['M']],\n",
    "    [3,dt['M']],\n",
    "    [3,dt['L']],\n",
    "    [3,dt['L']]\n",
    "    ])\n",
    "Y = np.array([\n",
    "    -1,\n",
    "    -1,\n",
    "    1,\n",
    "    1,\n",
    "    -1,\n",
    "    -1,\n",
    "    -1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    -1,\n",
    "    ]) \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "res = clf.predict(np.array([[2,dt['S']]]))\n",
    "print(\"res is {}\".format(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "提取所有文档中的词条并进行去重\n",
    "获取文档的所有类别\n",
    "计算每个类别中的文档数目\n",
    "对每篇训练文档: \n",
    "    对每个类别: \n",
    "        如果词条出现在文档中-->增加该词条的计数值(for循环或者矩阵相加)\n",
    "        增加所有词条的计数值（此类别下词条总数）\n",
    "对每个类别: \n",
    "    对每个词条: \n",
    "        将该词条的数目除以总词条数目得到的条件概率(P(词条|类别))\n",
    "返回该文档属于每个类别的条件概率(P(类别|文档的所有词条))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。\n",
    "如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。\n",
    "对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。\n",
    "\"\"\"\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    创建数据集\n",
    "    :return: 单词列表postingList, 所属类别classVec\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], #[0,0,1,1,1......]\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not\n",
    "    return postingList, classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocabSet = set([])  # create empty set\n",
    "    for document in dataSet:\n",
    "        # 操作符 | 用于求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)  # union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocabList: 所有单词集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    returnVec = [0] * len(vocabList)# [0,0......]\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    "\n",
    "def _trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据原版\n",
    "    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]\n",
    "    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 文件数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率，即trainCategory中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词出现次数列表\n",
    "    p0Num = zeros(numWords) # [0,0,0,.....]\n",
    "    p1Num = zeros(numWords) # [0,0,0,.....]\n",
    "\n",
    "    # 整个数据集单词出现总数\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i] #[0,1,1,....]->[0,1,1,...]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            # 如果不是侮辱性文件，则计算非侮辱性文件中出现的侮辱性单词的个数\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表\n",
    "    # 即 在1类别下，每个单词出现次数的占比\n",
    "    p1Vect = p1Num / p1Denom# [1,2,3,5]/90->[1/90,...]\n",
    "    # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    # 即 在0类别下，每个单词出现次数的占比\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据优化版本\n",
    "    :param trainMatrix: 文件单词矩阵\n",
    "    :param trainCategory: 文件对应的类别\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 总文件数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 总单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词出现次数列表\n",
    "    # p0Num 正常的统计\n",
    "    # p1Num 侮辱的统计 \n",
    "    # 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1\n",
    "    p0Num = ones(numWords)#[0,0......]->[1,1,1,1,1.....]\n",
    "    p1Num = ones(numWords)\n",
    "\n",
    "    # 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）\n",
    "    # p0Denom 正常的统计\n",
    "    # p1Denom 侮辱的统计\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            # 累加辱骂词的频次\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对每篇文章的辱骂的频次 进行统计汇总\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    使用算法: \n",
    "        # 将乘法转换为加法\n",
    "        乘法: P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法: P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param pClass1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是: 这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "def testingNB():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    # 2. 创建单词集合\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # 4. 训练数据\n",
    "    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    # 5. 测试数据\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "testingNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
